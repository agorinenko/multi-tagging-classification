{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/gorinenko/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/gorinenko/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import ssl\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, hamming_loss\n",
    "from sklearn.metrics import hamming_loss\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import catboost as ctb\n",
    "from catboost.utils import eval_metric\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "    \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import lightning as L\n",
    "    \n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Постановка задачи\n",
    "\n",
    "В второй части w2v_base_line.ipynb мы составили базовый pipeline обучение модели CatBoostClassifier на базе признаков, извлеченных с использованием Word2Vec. В этом документе попробуем использовать рекурентные сети для предсказания меток-тегов и сравним скорость обучения модели и ее предсказательную способность. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Предобработка текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples count 100000\n"
     ]
    }
   ],
   "source": [
    "file_path = 'data/stackoverflow_posts.csv'\n",
    "# file_path = 'data/stackexchange_posts.csv'\n",
    "\n",
    "nrows = 100_000\n",
    "raw_df = pd.read_csv(file_path)\n",
    "df = raw_df[~raw_df['Tags'].isna()]\n",
    "\n",
    "df = df.iloc[:nrows, :]\n",
    "df.reset_index(inplace=True)\n",
    "# Всего - 912090\n",
    "\n",
    "print(f'samples count {df.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_tags(value):\n",
    "    tags = value.replace('<', '').split('>')\n",
    "    return [tag for tag in tags if tag]\n",
    "\n",
    "    \n",
    "df[\"Tags\"] = df[\"Tags\"].apply(lambda x: parse_tags(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('russian'))\n",
    "punctuation = set(string.punctuation)\n",
    "\n",
    "\n",
    "class TextPreProcessor:\n",
    "    def __init__(self, tokenizer, stemmer=None, morph=None):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.stemmer = stemmer\n",
    "        self.morph = morph\n",
    "\n",
    "\n",
    "    def tokenize(self, text: str):      \n",
    "        text = text.lower()\n",
    "      \n",
    "        doc = BeautifulSoup(text, 'lxml')\n",
    "        text = doc.text\n",
    "        \n",
    "        tokens = self.tokenizer.tokenize(text)\n",
    "        \n",
    "        words = [word for word in tokens if word not in stop_words and word not in punctuation]\n",
    "        \n",
    "        if self.morph:\n",
    "            words = [self.morph.parse(word)[0].normal_form for word in words]\n",
    "\n",
    "        if self.stemmer:\n",
    "            words = [self.stemmer.stem(word) for word in words]\n",
    "\n",
    "        return words\n",
    "    \n",
    "class NltkTokenizer:    \n",
    "    def tokenize(self, text: str):      \n",
    "        return list(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построим словарь, который сопоставляет словам некие индексы. Мы используем специальный токен **unk**, который будет возвращен, если слово отсутствует в словаре. Внутри себя функция подсчитывает частоту появления каждого токена, потом слова сортируются по убыванию частоты и из уже отсортированного словаря строиться vocab. Таким образом самые часто встречаемые слова будут в начале(специальный токен **unk** имеет индекс 0). В словаре также имеется токен **pad**, который дополняет предложение до определенной длины, если оно меньше, например, если мы условились, что длина последовательности равна 10, то предложение 'here is the an example' будет закодировано так 'here is the an example pad pad pad pad pad'. Только на месте чисел должны стоять их индексы в словаре.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/g5/8xrz2mg176j1yxttghxsl4gc0000gn/T/ipykernel_29557/2654832454.py:15: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  doc = BeautifulSoup(text, 'lxml')\n"
     ]
    }
   ],
   "source": [
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "\n",
    "def preprocessor(text):\n",
    "    if not isinstance(text, str):\n",
    "          return text\n",
    "      \n",
    "    tokenizer = TextPreProcessor(tokenizer=NltkTokenizer())    \n",
    "    words = tokenizer.tokenize(text)\n",
    "    return words\n",
    "    \n",
    "def yield_tokens(data_iter):\n",
    "    for tokens in data_iter:\n",
    "        yield preprocessor(tokens)\n",
    "        \n",
    "df['Body_cleaned'] = df['Body'].apply(lambda x: preprocessor(x))\n",
    "        \n",
    "vocab = build_vocab_from_iterator(yield_tokens(df['Body_cleaned']), specials=['<unk>', '<pad>'])\n",
    "vocab.set_default_index(vocab['<unk>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер словаря 316302\n",
      "Индексы слов [252, 10873, 0, 0]\n",
      "Слово с индексом 100 - \"value=\"\n"
     ]
    }
   ],
   "source": [
    "print(f'Размер словаря {len(vocab)}')\n",
    "print(f'Индексы слов {vocab([\"javascript\", \"принтер\", \"abra-cadabra\", \"<unk>\"])}')\n",
    "print(f'Слово с индексом 100 - \"{vocab.lookup_token(100)}\"')\n",
    "\n",
    "PAD_INDEX=vocab['<pad>']\n",
    "UNK_INDEX=vocab['<unk>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_label = MultiLabelBinarizer()\n",
    "Y = multi_label.fit_transform(list(df[\"Tags\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наконец, создадим WordDataset и DataLoader с использованием функции collate_batch, которая будет формировать пакеты наших данных. Обратите внимание, что архитектура RNN требует, чтобы все предложения в пакете данных для обучения на каком-то шаге имели одинаковую длину. Поэтому в процессе формирования пакетов преобразуем токены текста в индексы подготовленного словаря, а затем дополним последовательности индексом специального символа **pad**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class WordDataset:\n",
    "    def __init__(self, data,  encode_labels=None):\n",
    "        self.data = np.array(data['Body_cleaned'])\n",
    "        self.encode_labels = None if encode_labels is None else np.array(encode_labels)\n",
    "        assert len(self.data) == len(self.encode_labels)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        if self.encode_labels is None:\n",
    "            return None, self.data[idx]\n",
    "        \n",
    "        return self.encode_labels[idx], self.data[idx]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "random_state=42\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "text_pipeline = lambda x: vocab(x)\n",
    "\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list, offsets = None, [], []\n",
    "    \n",
    "    # Формируем списки тензоров\n",
    "    for _label, _text in batch:\n",
    "        # Режим тренировки\n",
    "        if _label is not None:\n",
    "            if label_list is None:\n",
    "                label_list = []\n",
    "            label_list.append(torch.FloatTensor(_label))\n",
    "        \n",
    "        processed_text = torch.tensor(vocab(_text), dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "        \n",
    "        offsets.append(processed_text.size(0))\n",
    "    \n",
    "    # Преобразуем списки тензоров в тензор и выравниваем последовательности\n",
    "    if label_list:\n",
    "        label_list = torch.FloatTensor(np.array(label_list)).to(device)\n",
    "        \n",
    "    offsets = torch.tensor(np.array(offsets), dtype=torch.int64).to(device)\n",
    "    text_list = pad_sequence(text_list, padding_value=PAD_INDEX).permute(1, 0).to(device)\n",
    "    \n",
    "    # Сортируем\n",
    "    offsets, ordering = torch.sort(offsets, dim=0, descending=True)\n",
    "    text_list = text_list[ordering]\n",
    "    if label_list is not None:\n",
    "        label_list = label_list[ordering].to(device)\n",
    "   \n",
    "        \n",
    "    return text_list.to(device), label_list\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df, Y, test_size=0.1, random_state=random_state, shuffle=False)\n",
    "# , num_workers=11, persistent_workers=True\n",
    "train_loader = DataLoader(WordDataset(X_train, y_train), batch_size=8, shuffle=True, drop_last=True, collate_fn=collate_batch, num_workers=11, persistent_workers=True)\n",
    "valid_loader = DataLoader(WordDataset(X_test, y_test), batch_size=8, shuffle=False, drop_last=True, collate_fn=collate_batch, num_workers=11, persistent_workers=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Определение модели\n",
    "\n",
    "Для предсказывания меток попробуем использовать рекурентные нейронные сети."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 200\n",
    "RNN_HIDDEN_DIM = 100\n",
    "RNN_NUM_LAYERS = 2\n",
    "\n",
    "class TextClassificationModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(num_embeddings=len(vocab), embedding_dim=EMBEDDING_DIM, padding_idx = PAD_INDEX)\n",
    "        self.rnn = nn.RNN(input_size=EMBEDDING_DIM, hidden_size=RNN_HIDDEN_DIM, num_layers=RNN_NUM_LAYERS, batch_first=True)\n",
    "        self.linear = nn.Linear(RNN_HIDDEN_DIM, len(multi_label.classes_))\n",
    "        \n",
    "    def forward(self, X_batch):\n",
    "        embeddings = self.embedding(X_batch)\n",
    "        output, _ = self.rnn(embeddings)\n",
    "        \n",
    "        return self.linear(output[:,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import lightning as L\n",
    "\n",
    "\n",
    "class TextClassificationLightningModule(L.LightningModule):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        self.model = model\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        \n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        X, y = batch\n",
    "        logits = self.model(X)\n",
    "        \n",
    "        prob = self.sigmoid(logits)        \n",
    "        preds = torch.round(prob)\n",
    "        \n",
    "        return prob, preds\n",
    "\n",
    "    def training_step(self, batch, *args):\n",
    "        X, y = batch\n",
    "        logits = self.model(X)\n",
    "        \n",
    "        loss = F.cross_entropy(logits, y)        \n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        X, y = batch\n",
    "        logits = self.model(X)\n",
    "        \n",
    "        prob = self.sigmoid(logits)        \n",
    "        preds = torch.round(prob)\n",
    "        \n",
    "        test_loss = F.mse_loss(preds, y)\n",
    "        self.log(\"test_loss\", test_loss)\n",
    "        \n",
    "    # def validation_step(self, batch, batch_idx):\n",
    "    #     X, y = batch\n",
    "    #     logits = self.model(X)\n",
    "        \n",
    "    #     prob = self.sigmoid(logits)        \n",
    "    #     preds = torch.round(prob)\n",
    "        \n",
    "    #     val_loss = F.mse_loss(preds, y)\n",
    "    #     self.log(\"val_loss\", val_loss)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gorinenko/src/multi_tagging_classification/.venv/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:198: Attribute 'model' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['model'])`.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/gorinenko/src/multi_tagging_classification/.venv/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:67: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "\n",
      "  | Name    | Type                    | Params\n",
      "----------------------------------------------------\n",
      "0 | model   | TextClassificationModel | 63.5 M\n",
      "1 | sigmoid | Sigmoid                 | 0     \n",
      "----------------------------------------------------\n",
      "63.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "63.5 M    Total params\n",
      "254.132   Total estimated model params size (MB)\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/spawn.py\", line 132, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'WordDataset' on <module '__main__' (built-in)>\n",
      "/Users/gorinenko/src/multi_tagging_classification/.venv/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "model = TextClassificationLightningModule(TextClassificationModel())\n",
    "trainer = L.Trainer(max_epochs=1, default_root_dir=\"data/models/\", profiler=\"simple\")\n",
    "# trainer.fit(model, train_loader, valid_loader)\n",
    "trainer.fit(model, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gorinenko/src/multi_tagging_classification/.venv/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d9d24db2fc146689f9a10b87ef46a07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_loss           0.0273505300283432\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 0.0273505300283432}]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(model, valid_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Оценка модели\n",
    "\n",
    "Оценим метрики scikit learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Предсказания модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_df = df[df['Tags'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Текст: \n",
      "<p>Всем привет!) У меня установлен ocStore 2.3.0.2.3.\n",
      "Я хочу, чтобы, когда клиент авторизовался, было написан его ник, а не \"личный кабинет\". Очень долго уже ищу, не могу найти. Нашел вот такую строку, но куда ее вставлять не знаю:\n",
      "$data['text_username'] = $this->customer->getFirstName();</p>\n",
      "\n",
      "<p>Спасибо за любую помощь!)</p>\n",
      "\n",
      "\n",
      "Предсказанные теги: \n",
      "[('.net', 'ajax', 'android', 'c', 'c#', 'c++', 'css', 'delphi', 'gwt', 'html', 'html5', 'ide', 'java', 'javascript', 'joomla', 'jquery', 'linux', 'mysql', 'pascal', 'php', 'python', 'sql', 'sql-server', 'ubuntu', 'visual-basic', 'visual-studio', 'winapi', 'windows', 'winforms', 'wpf', 'алгоритм', 'ассемблер', 'база-данных', 'книги', 'массивы', 'регулярные-выражения')]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "valid_df = raw_df[~raw_df['Tags'].isna()]\n",
    "valid_df.reset_index(inplace=True)\n",
    "\n",
    "\n",
    "row_num = random.randint(0, valid_df['Body'].shape[0])\n",
    "text = valid_df['Body'][row_num]\n",
    "tokens = preprocessor(text)\n",
    "\n",
    "\n",
    "print(f'Текст: \\n{text}\\n')\n",
    "\n",
    "processed_text = torch.tensor(vocab(tokens), dtype=torch.int64)\n",
    "processed_text = pad_sequence([processed_text], padding_value=PAD_INDEX).permute(1, 0).to(device)\n",
    "model.model.eval()\n",
    "with torch.no_grad():\n",
    "    logits = model.model(processed_text)\n",
    "    \n",
    "    prob = torch.nn.Sigmoid()(logits)        \n",
    "    preds = torch.round(prob)\n",
    "\n",
    "\n",
    "labels = multi_label.inverse_transform(preds.reshape(1, -1))\n",
    "print(f'Предсказанные теги: \\n{labels}\\n')\n",
    "\n",
    "len(labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels[0])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
